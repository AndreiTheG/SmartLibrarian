# ğŸ“š Smart Librarian - DavaX - AI with RAG & Tool Completion 

An intelligent chatbot acting as a digital librarian, allowing interaction through text, voice, and LLMs, using a modern interface and scalable backend.

---

## ğŸ§° Technologies Used

- **fastapi** â€“ The main web framework
- **uvicorn** â€“ Runs the FastAPI app as a local web server
- **pydantic** â€“ Data validation & serialization
- **sqlalchemy** â€“ Providing SQL and ORM (Object Relational Mapping) features for database access  
- **chromadb** â€“ Storing and querying the embeddings for LLM apps using RAG approach  
- **openai** â€“ Providing tools to interact with OpenAI API  
- **speech_recognition** â€“ Used for speech-to-text conversion  
- **pyttsx3** â€“ Used for text-to-speech conversion  

---

## ğŸ“˜ Tutorial for Using the Librarian Chatbot Application

### ğŸ”¹ Step 1 â€“ Clone the repository in PyCharm

Clone the repo from GitHub into PyCharm using the command:

```bash
git clone https://github.com/AndreiTheG/SmartLibrarian.git
```

---

### ğŸ”¹ Step 2 â€“ Create a virtual environment and install required packages

```bash
pip install fastapi uvicorn pydantic sqlalchemy chromadb openai speech_recognition pyttsx3
```

---

### ğŸ”¹ Step 3 â€“ Install frontend frameworks (React + Vite)

Make sure you have Node.js installed, then run:

```bash
npm create vite@latest frontend --template react
cd frontend
npm install
```

---

### ğŸ”¹ Step 4 â€“ Set the OpenAI API key

Set the key in your terminal:

```bash
export OPENAI_API_KEY=your_api_key_here
```

---

## ğŸ§  Application Structure

### ğŸ”¸ Step 5.1 â€“ Running `main.py`

Run the FastAPI server and define the `/chat` route.

### ğŸ”¸ Pasul 5.2 â€“ Modele Pydantic

Create the `model` folder and add:

- `ChatRequest.py`
- `SummaryBook.py`
- `SummaryBookModel.py`

### ğŸ”¸ Step 5.3 â€“ `Database.py` Script

Create the SQLite database with book data.

### ğŸ”¸ Step 5.4 â€“ Define API Routes

- `POST` â€“ Insert book data and messages
- `GET` â€“ Retrieve data by ID

### ğŸ”¸ Pasul 5.5 â€“ Run `load_db_to_chroma.py`

```bash
python backend/chatbot/load_db_to_chroma.py
```

### ğŸ”¸ Step 5.6 â€“ Start the CLI

```bash
python -m backend.chatbot.cli_bot
```

---

## ğŸŒ Frontend

### ğŸ”¸ Step 6.1 â€“ Install Axios

```bash
npm install axios
```

### ğŸ”¸ Step 6.2 â€“ Create Chat.jsx + integrate with App.jsx

Use the endpoint `http://localhost:8000/chat`.

---

### ğŸ”¸ Step 7 â€“ Enable CORS in FastAPI

Add in `main.py`:

```python
origins = ["http://localhost:5173"]
```

---

## ğŸš€ Running the Application

### â–¶ï¸ Backend

```bash
uvicorn backend.main:app --reload
```

### â–¶ï¸ Frontend

```bash
npm run dev
```

Access [http://localhost:5173](http://localhost:5173)

---

## ğŸ“š Main Features

- ğŸ” Insert and retrieve book summaries 
- ğŸ¤– Chatbot interaction (text + voice)  
- ğŸ“ˆ Vectorization in ChromaDB  
- ğŸ§  Generate summaries and recommendations using GPT  

---

## âœ… Recommendations

- Run backend and frontend in parallel 
- Reload Chroma database if you add new books  
- The app is easily extendable for other LLM/UI integrations

## Application Interface:
![Alt text](./Chatbot_interface.png)
